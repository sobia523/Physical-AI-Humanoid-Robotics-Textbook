# Data Model: Vision-Language-Action (VLA) Robotics Module

**Date**: 2025-12-15
**Feature**: `004-vla-robotics-module`

This document outlines the key data entities for the VLA Robotics Module. These entities represent the information that flows through the voice-to-action pipeline.

## Entity: VoiceCommand

Represents the user's spoken instruction, which is the initial input to the system.

-   **Description**: A single, self-contained voice command from the user.
-   **Attributes**:
    -   `audio_data` (Raw Audio): The raw audio buffer captured from the microphone.
    -   `transcribed_text` (String): The text representation of the audio, as transcribed by the voice-to-text service.
-   **State Transitions**:
    1.  `Unprocessed`: The initial state when audio is first captured.
    2.  `Transcribed`: The state after the audio has been successfully converted to text.
    3.  `FailedTranscription`: The state if the transcription service fails or returns an empty result.

## Entity: TaskPlan

An ordered sequence of actions generated by the LLM based on the user's command.

-   **Description**: Represents the high-level plan that the robot will execute.
-   **Attributes**:
    -   `id` (String): A unique identifier for the plan.
    -   `source_command` (String): The transcribed text that this plan was generated from.
    -   `actions` (List of `Action`): The sequence of actions to be executed.
    -   `status` (String): The current status of the plan (e.g., `Pending`, `Executing`, `Completed`, `Failed`).

## Entity: Action

A single, atomic step within a `TaskPlan`. Each action corresponds to a specific capability of the robot.

-   **Description**: A command for the robot to perform a single operation, like navigating to a location or grasping an object.
-   **Attributes**:
    -   `type` (String): The type of action to be performed (e.g., `Navigate`, `Grasp`, `Release`).
    -   `parameters` (Dictionary): The parameters for the action, which vary depending on the action type. For example:
        -   For a `Navigate` action, parameters might include `target_location: [x, y, z]`.
        -   For a `Grasp` action, parameters might include `target_object_id: "red_box"`.
    -   `status` (String): The status of the individual action (e.g., `Pending`, `InProgress`, `Success`, `Failure`).

## Entity: SimulatedObject

Represents a detectable and manipulable object within the Gazebo or Unity simulation environment.

-   **Description**: An object in the world that the robot can perceive and interact with.
-   **Attributes**:
    -   `id` (String): A unique identifier for the object (e.g., "red_box", "blue_bottle").
    -   `type` (String): The class of the object (e.g., "box", "bottle", "cube").
    -   `position` (Vector3): The object's current position (x, y, z) in the world frame.
    -   `orientation` (Quaternion): The object's current orientation in the world frame.
    -   `properties` (Dictionary): Additional properties, such as `color`, `size`, etc.

## Relationships

-   A `VoiceCommand` is processed to generate one `TaskPlan`.
-   A `TaskPlan` consists of one or more `Actions`.
-   An `Action` often targets a `SimulatedObject`.
-   The perception system is responsible for identifying `SimulatedObject`s and making their information available to the planning and execution systems.
